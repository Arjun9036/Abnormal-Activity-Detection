{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfbc97d-c066-4d2e-8179-57b36a7fdba9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d728d5f6-fc6a-44c7-94f5-070bb3b4214b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T17:17:08.839513Z",
     "start_time": "2025-02-16T17:17:08.766548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 211s/step - accuracy: 0.2500 - loss: 1.1915 - val_accuracy: 0.0000e+00 - val_loss: 1.2184\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 143s/step - accuracy: 0.8750 - loss: 0.6989 - val_accuracy: 0.0000e+00 - val_loss: 1.2698\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 126s/step - accuracy: 1.0000 - loss: 0.3766 - val_accuracy: 0.0000e+00 - val_loss: 1.5217\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 132s/step - accuracy: 1.0000 - loss: 0.1900 - val_accuracy: 0.0000e+00 - val_loss: 1.9230\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 138s/step - accuracy: 1.0000 - loss: 0.1049 - val_accuracy: 0.0000e+00 - val_loss: 2.2826\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13s/step\n",
      "Predicted activity: Arrest\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Global Configurations\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 3  \n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "\n",
    "# Dataset Preparation\n",
    "def extract_frames(video_path, output_size=IMAGE_SIZE):\n",
    "    \"\"\"Extract frames from a video and resize them.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def preprocess_data(video_paths, labels, frame_count=FRAME_COUNT):\n",
    "    \"\"\"Preprocess videos into fixed frame count sequences.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        frames = extract_frames(video_path)\n",
    "        if len(frames) >= frame_count:\n",
    "            X.append(frames[:frame_count])\n",
    "            y.append(labels[i])\n",
    "    return np.array(X), to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "def build_cnn():\n",
    "    \"\"\"Build a CNN model for spatial feature extraction.\"\"\"\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# RNN Model for Temporal Analysis\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    \"\"\"Build an RNN for temporal feature analysis.\"\"\"\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),  \n",
    "        LSTM(256, return_sequences=True),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Combined Spatio-Temporal Model\n",
    "def build_spatio_temporal_model(cnn, rnn, frame_count, num_classes):\n",
    "    \"\"\"Combine CNN and RNN for spatio-temporal analysis.\"\"\"\n",
    "    video_input = Input(shape=(frame_count, 224, 224, 3))\n",
    "    cnn_output = TimeDistributed(cnn)(video_input)\n",
    "    ##print cnn_output\n",
    "    rnn_output = rnn(cnn_output)\n",
    "    ##print rnn o/p\n",
    "    model = Model(inputs=video_input, outputs=rnn_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example Dataset\n",
    "video_paths = ['Downloads/dataset/Burglary/Burglary009_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary059_x264.mp4', \n",
    "               'Downloads/dataset/Burglary/Burglary098_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse016_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse018_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse019_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse041_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest015_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest011_x264.mp4',\n",
    "              'Downloads/dataset/Arrest/Arrest042_x264.mp4'\n",
    "              ]  \n",
    "labels = [0, 0, 0, 1, 1, 1, 1, 2, 2, 2]  \n",
    "\n",
    "# Preprocess Data\n",
    "X, y = preprocess_data(video_paths, labels, frame_count=FRAME_COUNT)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build Models\n",
    "cnn = build_cnn()\n",
    "rnn = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "model = build_spatio_temporal_model(cnn, rnn, FRAME_COUNT, NUM_CLASSES)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the Model\n",
    "model.save('activity_recognition_model.keras')\n",
    "\n",
    "# Real-Time Detection\n",
    "def predict_activity(model, video_path):\n",
    "    \"\"\"Predict the activity in a video.\"\"\"\n",
    "    frames = extract_frames(video_path)\n",
    "    frames = frames[:FRAME_COUNT]\n",
    "    frames = np.expand_dims(frames, axis=0)\n",
    "    prediction = model.predict(frames)\n",
    "    return np.argmax(prediction)\n",
    "\n",
    "# Test Prediction\n",
    "test_video = 'Downloads/dataset/Arrest/Arrest042_x264.mp4'\n",
    "result = predict_activity(model, test_video)\n",
    "print(f\"Predicted activity: {CLASS_NAMES[result]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d758ff-9f89-40b4-a819-fb5dd5ce965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask\n",
      "  Downloading flask-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: jinja2>=3.1.2 in ./myenv/lib/python3.10/site-packages (from flask) (3.1.5)\n",
      "Collecting itsdangerous>=2.2.0\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in ./myenv/lib/python3.10/site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: click>=8.1.3 in ./myenv/lib/python3.10/site-packages (from flask) (8.1.8)\n",
      "Collecting blinker>=1.9.0\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in ./myenv/lib/python3.10/site-packages (from flask) (3.0.2)\n",
      "Installing collected packages: itsdangerous, blinker, flask\n",
      "Successfully installed blinker-1.9.0 flask-3.1.1 itsdangerous-2.2.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/arjungoyal/myenv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bac1a71-caf6-4133-844e-66a462fbd790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjungoyal/myenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "import logging\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from flask import Flask, render_template_string\n",
    "from PyQt5.QtWidgets import QApplication, QLabel, QPushButton, QWidget, QVBoxLayout\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "from PyQt5.QtCore import QTimer\n",
    "from tensorflow.keras.models import load_model\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# --------- Logging Setup ----------\n",
    "logging.basicConfig(filename=\"alerts.log\", level=logging.INFO, \n",
    "                    format='%(asctime)s - %(message)s')\n",
    "\n",
    "# --------- Alert System ----------\n",
    "def send_email_alert(subject, body):\n",
    "    try:\n",
    "        sender = \"youremail@example.com\"\n",
    "        receiver = \"receiver@example.com\"\n",
    "        password = \"your_app_password\"\n",
    "\n",
    "        msg = MIMEText(body)\n",
    "        msg['Subject'] = subject\n",
    "        msg['From'] = sender\n",
    "        msg['To'] = receiver\n",
    "\n",
    "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
    "            server.login(sender, password)\n",
    "            server.send_message(msg)\n",
    "        logging.info(\"Email alert sent.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to send alert email: {e}\")\n",
    "\n",
    "\n",
    "# --------- Preprocessing Function ----------\n",
    "def preprocess_frame(frame, target_size=(224, 224)):\n",
    "    frame_resized = cv2.resize(frame, target_size)\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    frame_normalized = frame_rgb.astype(\"float32\") / 255.0\n",
    "    return frame_normalized\n",
    "\n",
    "\n",
    "# --------- Real-Time Activity Detector ----------\n",
    "class RealTimeActivityDetector:\n",
    "    def __init__(self, model_path, sequence_length=30):\n",
    "        self.model = load_model(model_path)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.frame_buffer = deque(maxlen=sequence_length)\n",
    "        self.labels = ['burglary', 'abuse', 'arrest']  # update with your model's classes\n",
    "\n",
    "    def predict(self, frame):\n",
    "        preprocessed = preprocess_frame(frame)\n",
    "        self.frame_buffer.append(preprocessed)\n",
    "\n",
    "        if len(self.frame_buffer) == self.sequence_length:\n",
    "            sequence = np.expand_dims(np.array(self.frame_buffer), axis=0)\n",
    "            predictions = self.model.predict(sequence, verbose=0)[0]\n",
    "            label_idx = np.argmax(predictions)\n",
    "            confidence = predictions[label_idx]\n",
    "            return self.labels[label_idx], confidence\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# --------- PyQt GUI ----------\n",
    "class SurveillanceApp(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Real-Time Activity Detection\")\n",
    "        self.setGeometry(100, 100, 800, 600)\n",
    "\n",
    "        self.video_label = QLabel(\"Video Feed\")\n",
    "        self.start_btn = QPushButton(\"Start Detection\")\n",
    "        self.stop_btn = QPushButton(\"Stop Detection\")\n",
    "\n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.video_label)\n",
    "        layout.addWidget(self.start_btn)\n",
    "        layout.addWidget(self.stop_btn)\n",
    "        self.setLayout(layout)\n",
    "\n",
    "        self.cap = None\n",
    "        self.timer = QTimer()\n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        self.detector = RealTimeActivityDetector(\"activity_recognition_model.keras\")\n",
    "        self.last_alert_time = datetime.min\n",
    "\n",
    "        self.start_btn.clicked.connect(self.start_camera)\n",
    "        self.stop_btn.clicked.connect(self.stop_camera)\n",
    "\n",
    "    def start_camera(self):\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.timer.start(30)  # roughly 30 FPS\n",
    "\n",
    "    def update_frame(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            return\n",
    "\n",
    "        label, confidence = self.detector.predict(frame)\n",
    "        if label and confidence > 0.8:\n",
    "            cv2.putText(frame, f\"{label.upper()} ({confidence*100:.1f}%)\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            # Alert every 10 sec max\n",
    "            if (datetime.now() - self.last_alert_time).seconds > 10:\n",
    "                logging.info(f\"Alert: {label} detected with confidence {confidence:.2f}\")\n",
    "                send_email_alert(\"ALERT: Suspicious Activity\", f\"{label} detected with {confidence*100:.2f}% confidence\")\n",
    "                self.last_alert_time = datetime.now()\n",
    "        else:\n",
    "            cv2.putText(frame, \"Normal\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = rgb_frame.shape\n",
    "        qt_image = QImage(rgb_frame.data, w, h * ch, QImage.Format_RGB888)\n",
    "        self.video_label.setPixmap(QPixmap.fromImage(qt_image))\n",
    "\n",
    "    def stop_camera(self):\n",
    "        self.timer.stop()\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "        self.video_label.clear()\n",
    "\n",
    "\n",
    "# --------- Flask App for LAN Access ----------\n",
    "flask_app = Flask(__name__)\n",
    "\n",
    "@flask_app.route('/')\n",
    "def index():\n",
    "    return render_template_string(\"<h2>Real-Time Activity Detection Running</h2>\")\n",
    "\n",
    "\n",
    "def run_flask():\n",
    "    try:\n",
    "        flask_app.run(host='0.0.0.0', port=5050)  # safer default port\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Flask server error: {e}\")\n",
    "\n",
    "\n",
    "# --------- Main Entry ----------\n",
    "if __name__ == \"__main__\":\n",
    "    flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
    "    flask_thread.start()\n",
    "\n",
    "    qt_app = QApplication(sys.argv)\n",
    "    window = SurveillanceApp()\n",
    "    window.show()\n",
    "    sys.exit(qt_app.exec_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783531a-4306-4369-9c63-0d40d1c661ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5bdf74c-9df5-4dd2-a06e-2e1f3fd42200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\n",
      "✅ CNN Output shape: (1, 30, 2048) (frame_count x feature_vector_size)\n",
      "CNN Output Vector (1st frame):\n",
      " [0.00599981 2.078606   0.45295173 ... 1.1780607  0.24603099 0.9764378 ]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\n",
      "✅ RNN Output shape: (1, 3) (1 x num_classes)\n",
      "RNN Output Vector:\n",
      " [[0.27041426 0.38492414 0.34466153]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step\n",
      "\n",
      "🎯 Final Model Prediction: Burglary\n",
      "Final Model Output Vector:\n",
      " [[0.8912993  0.08720166 0.02149893]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import TimeDistributed, Input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Constants\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# --- Load and Build Components Again (Same Architecture Required) ---\n",
    "\n",
    "# Load trained full model\n",
    "full_model = load_model('activity_recognition_model.keras')\n",
    "\n",
    "# Rebuild CNN\n",
    "def build_cnn():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "cnn_model = build_cnn()\n",
    "\n",
    "# Rebuild RNN\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        LSTM(256, return_sequences=True),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "rnn_model = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "\n",
    "# --- Frame Extraction ---\n",
    "def extract_frames(video_path, output_size=(224, 224), frame_count=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "        if len(frames) == frame_count:\n",
    "            break\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "# --- Load test video ---\n",
    "video_path = 'Downloads/dataset/Burglary/Burglary059_x264.mp4'\n",
    "frames = extract_frames(video_path)\n",
    "frames = frames[:FRAME_COUNT]\n",
    "frames_batch = np.expand_dims(frames, axis=0)  # (1, 30, 224, 224, 3)\n",
    "\n",
    "# --- Get CNN Output ---\n",
    "cnn_features = np.array([cnn_model.predict(frames_batch[0])])  # shape: (30, 2048)\n",
    "print(f\"\\n✅ CNN Output shape: {cnn_features.shape} (frame_count x feature_vector_size)\")\n",
    "print(\"CNN Output Vector (1st frame):\\n\", cnn_features[0][0])\n",
    "\n",
    "# --- Get RNN Output ---\n",
    "rnn_input = np.expand_dims(cnn_features[0], axis=0)  # (1, 30, 2048)\n",
    "rnn_output = rnn_model.predict(rnn_input)\n",
    "print(f\"\\n✅ RNN Output shape: {rnn_output.shape} (1 x num_classes)\")\n",
    "print(\"RNN Output Vector:\\n\", rnn_output)\n",
    "\n",
    "# --- Final Model Prediction ---\n",
    "prediction = full_model.predict(frames_batch)\n",
    "predicted_class = np.argmax(prediction)\n",
    "print(f\"\\n🎯 Final Model Prediction: {CLASS_NAMES[predicted_class]}\")\n",
    "print(\"Final Model Output Vector:\\n\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb321f-13cd-4495-a230-374497d77cf1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CNN & RNN outputs CSV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d75a0c3b-82bb-461f-8ec4-998ec1b10f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary089_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary010_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary083_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary059_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary098_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary062_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary009_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting036_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting037_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting030_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting005_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting013_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting048_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting022_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest015_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest030_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest029_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest042_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest043_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest033_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest011_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_606_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_781_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_189_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_941_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_365_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_696_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_010_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse019_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse018_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse041_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse032_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse044_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse016_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse022_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault002_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault018_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault051_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault029_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault011_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault045_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault033_x264.mp4\n",
      "\n",
      "✅ All predictions logged to: activity_predictions_log.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Constants\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# Paths\n",
    "VIDEO_DIR = 'Downloads/dataset'  # Root dataset directory\n",
    "OUTPUT_CSV = 'activity_predictions_log.csv'\n",
    "\n",
    "# Load trained full model\n",
    "full_model = load_model('activity_recognition_model.keras')\n",
    "\n",
    "# Rebuild CNN\n",
    "def build_cnn():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "cnn_model = build_cnn()\n",
    "\n",
    "# Rebuild RNN\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(256, return_sequences=True),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "rnn_model = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "\n",
    "# Extract frames from video\n",
    "def extract_frames(video_path, output_size=(224, 224), frame_count=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "        if len(frames) == frame_count:\n",
    "            break\n",
    "    cap.release()\n",
    "    if len(frames) < frame_count:\n",
    "        print(f\"⚠️ Skipping {video_path}: Not enough frames ({len(frames)})\")\n",
    "        return None\n",
    "    return np.array(frames)\n",
    "\n",
    "# Walk through videos and make predictions\n",
    "def process_videos_and_log(video_dir, output_csv):\n",
    "    results = []\n",
    "\n",
    "    for root, _, files in os.walk(video_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                video_path = os.path.join(root, file)\n",
    "                print(f\"\\n📹 Processing: {video_path}\")\n",
    "                \n",
    "                frames = extract_frames(video_path)\n",
    "                if frames is None:\n",
    "                    continue\n",
    "                \n",
    "                frames_batch = np.expand_dims(frames, axis=0)  # (1, 30, 224, 224, 3)\n",
    "\n",
    "                # CNN Output\n",
    "                cnn_features = np.array([cnn_model.predict(frames_batch[0], verbose=0)])  # (1, 30, 2048)\n",
    "                \n",
    "                # RNN Output\n",
    "                rnn_input = np.expand_dims(cnn_features[0], axis=0)  # (1, 30, 2048)\n",
    "                rnn_output = rnn_model.predict(rnn_input, verbose=0)  # (1, num_classes)\n",
    "\n",
    "                # Final Model Output\n",
    "                final_output = full_model.predict(frames_batch, verbose=0)\n",
    "                predicted_class = np.argmax(final_output)\n",
    "\n",
    "                # Save record\n",
    "                results.append({\n",
    "                    \"Video Name\": file,\n",
    "                    \"Predicted Class\": CLASS_NAMES[predicted_class],\n",
    "                    \"CNN Vector (1st Frame)\": cnn_features[0][0][:5].tolist(),  # First 5 values for brevity\n",
    "                    \"RNN Output\": rnn_output[0].tolist(),\n",
    "                    \"Final Output\": final_output[0].tolist()\n",
    "                })\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(output_csv, mode='w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=results[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"\\n✅ All predictions logged to: {output_csv}\")\n",
    "\n",
    "# Run it\n",
    "process_videos_and_log(VIDEO_DIR, OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c41338a4-8840-4a9e-9064-000256318cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary089_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary010_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary083_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary059_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary098_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary062_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Burglary/Burglary009_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting036_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting037_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting030_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting005_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting013_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting048_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Fighting/Fighting022_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest015_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest030_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest029_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest042_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest043_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest033_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Arrest/Arrest011_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_606_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_781_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_189_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_941_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_365_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_696_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/normal/Normal_Videos_010_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse019_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse018_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse041_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse032_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse044_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse016_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Abuse/Abuse022_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault002_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault018_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault051_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault029_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault011_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault045_x264.mp4\n",
      "\n",
      "📹 Processing: Downloads/dataset/Assault/Assault033_x264.mp4\n",
      "\n",
      "✅ All predictions logged with CNN data from 5 frames → activity_predictions_log_detailed1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Constants\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# Paths\n",
    "VIDEO_DIR = 'Downloads/dataset'  # Root dataset directory\n",
    "OUTPUT_CSV = 'activity_predictions_log_detailed1.csv'\n",
    "\n",
    "# Load trained full model\n",
    "full_model = load_model('activity_recognition_model.keras')\n",
    "\n",
    "# Rebuild CNN\n",
    "def build_cnn():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "cnn_model = build_cnn()\n",
    "\n",
    "# Rebuild RNN\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(256, return_sequences=True),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "rnn_model = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "\n",
    "# Extract frames from video\n",
    "def extract_frames(video_path, output_size=(224, 224), frame_count=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "        if len(frames) == frame_count:\n",
    "            break\n",
    "    cap.release()\n",
    "    if len(frames) < frame_count:\n",
    "        print(f\"⚠️ Skipping {video_path}: Not enough frames ({len(frames)})\")\n",
    "        return None\n",
    "    return np.array(frames)\n",
    "\n",
    "# Walk through videos and make predictions\n",
    "def process_videos_and_log(video_dir, output_csv):\n",
    "    results = []\n",
    "\n",
    "    for root, _, files in os.walk(video_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                video_path = os.path.join(root, file)\n",
    "                print(f\"\\n📹 Processing: {video_path}\")\n",
    "                \n",
    "                frames = extract_frames(video_path)\n",
    "                if frames is None:\n",
    "                    continue\n",
    "                \n",
    "                frames_batch = np.expand_dims(frames, axis=0)  # (1, 30, 224, 224, 3)\n",
    "\n",
    "                # CNN Output\n",
    "                cnn_features = np.array([cnn_model.predict(frames_batch[0], verbose=0)])  # (1, 30, 2048)\n",
    "\n",
    "                # RNN Output\n",
    "                rnn_input = np.expand_dims(cnn_features[0], axis=0)  # (1, 30, 2048)\n",
    "                rnn_output = rnn_model.predict(rnn_input, verbose=0)  # (1, num_classes)\n",
    "\n",
    "                # Final Model Output\n",
    "                final_output = full_model.predict(frames_batch, verbose=0)\n",
    "                predicted_class = np.argmax(final_output)\n",
    "\n",
    "                # Prepare 5 CNN vectors for CSV (reduce size for readability)\n",
    "                cnn_vectors = cnn_features[0][:5]  # shape: (5, 2048)\n",
    "                cnn_vectors_flat = []\n",
    "                for i in range(5):\n",
    "                    # Add first 10 features from each frame for brevity\n",
    "                    cnn_vectors_flat.extend(cnn_vectors[i][:10].tolist())\n",
    "\n",
    "                results.append({\n",
    "                    \"Video Name\": file,\n",
    "                    \"Predicted Class\": CLASS_NAMES[predicted_class],\n",
    "                    \"RNN Output\": rnn_output[0].tolist(),\n",
    "                    \"Final Output\": final_output[0].tolist(),\n",
    "                    \"CNN_Frame1-5_First10Vals\": cnn_vectors_flat\n",
    "                })\n",
    "\n",
    "    # Prepare headers dynamically\n",
    "    cnn_headers = [f\"F{f+1}_CNN_feat_{i+1}\" for f in range(5) for i in range(10)]\n",
    "    headers = [\"Video Name\", \"Predicted Class\", \"RNN Output\", \"Final Output\"] + cnn_headers\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(output_csv, mode='w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        for row in results:\n",
    "            flat_row = {\n",
    "                \"Video Name\": row[\"Video Name\"],\n",
    "                \"Predicted Class\": row[\"Predicted Class\"],\n",
    "                \"RNN Output\": str(row[\"RNN Output\"]),\n",
    "                \"Final Output\": str(row[\"Final Output\"]),\n",
    "            }\n",
    "            for i, val in enumerate(row[\"CNN_Frame1-5_First10Vals\"]):\n",
    "                flat_row[cnn_headers[i]] = val\n",
    "            writer.writerow(flat_row)\n",
    "\n",
    "    print(f\"\\n✅ All predictions logged with CNN data from 5 frames → {output_csv}\")\n",
    "\n",
    "# Run it\n",
    "process_videos_and_log(VIDEO_DIR, OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c463da-78ce-4b17-a11b-ac8d96e4ac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16s/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model (if not already defined)\n",
    "model = load_model('activity_recognition_model.keras')  # Ensure model is saved properly\n",
    "\n",
    "# Define class names\n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "\n",
    "def plot_confusion_matrix(model, X_test, y_test, class_names):\n",
    "    \"\"\"Plot the confusion matrix.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)  # Convert softmax outputs to class indices\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the confusion matrix\n",
    "plot_confusion_matrix(model, X_test, y_test, CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303a367f-3573-47c7-b05e-906d017ffd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,574,339</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m3\u001b[0m)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2048\u001b[0m)       │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │     \u001b[38;5;34m2,574,339\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,379,915</span> (299.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,379,915\u001b[0m (299.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,108,931</span> (99.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,108,931\u001b[0m (99.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,120</span> (207.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m53,120\u001b[0m (207.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">52,217,864</span> (199.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m52,217,864\u001b[0m (199.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f90b3-009b-475d-87fd-837d8be69cc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Video classification using ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bce811b-3d4d-46b5-bf5c-e376074c4585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 47s/step - accuracy: 0.2500 - loss: 16.6396 - val_accuracy: 0.3333 - val_loss: 63.8408\n",
      "Epoch 2/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 30s/step - accuracy: 0.3889 - loss: 34.4294 - val_accuracy: 0.0000e+00 - val_loss: 18.0607\n",
      "Epoch 3/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 36s/step - accuracy: 0.6944 - loss: 8.2382 - val_accuracy: 0.3333 - val_loss: 41.2258\n",
      "Epoch 4/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 35s/step - accuracy: 0.5556 - loss: 13.5898 - val_accuracy: 0.3333 - val_loss: 33.0536\n",
      "Epoch 5/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 36s/step - accuracy: 0.7917 - loss: 2.5563 - val_accuracy: 0.3333 - val_loss: 59.9799\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21s/step\n",
      "Predicted activity: Burglary\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, TimeDistributed\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Global Configurations\n",
    "FRAME_COUNT=30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 3  \n",
    "\n",
    "# Dataset Preparation\n",
    "def extract_frames(video_path, output_size=IMAGE_SIZE):\n",
    "    \"\"\"Extract frames from a video and resize them.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def preprocess_data(video_paths, labels, start_frame=200, end_frame=230):\n",
    "    \"\"\"Preprocess videos into fixed frame count sequences from a specified range.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        frames = extract_frames(video_path)\n",
    "        if len(frames) > end_frame:  # Ensure there are enough frames\n",
    "            X.append(frames[start_frame:end_frame])  # Take frames from start_frame to end_frame\n",
    "            y.append(labels[i])\n",
    "    return np.array(X), to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "def build_cnn():\n",
    "    \"\"\"Build a CNN model for spatial feature extraction.\"\"\"\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Combined Spatio-Temporal Model without RNN\n",
    "def build_spatio_temporal_model(cnn, frame_count, num_classes):\n",
    "    \"\"\"Combine CNN for spatio-temporal analysis.\"\"\"\n",
    "    video_input = Input(shape=(frame_count, 224, 224, 3))\n",
    "    cnn_output = TimeDistributed(cnn)(video_input)\n",
    "    \n",
    "    # Flatten the output and add Dense layers for classification\n",
    "    x = tf.keras.layers.Flatten()(cnn_output)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=video_input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example Dataset\n",
    "video_paths = ['Downloads/dataset/Burglary/Burglary059_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary009_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary062_x264.mp4', \n",
    "               'Downloads/dataset/Burglary/Burglary083_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary089_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse016_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse018_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse022_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse032_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse041_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse044_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest030_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest033_x264.mp4',\n",
    "              'Downloads/dataset/Arrest/Arrest015_x264.mp4',\n",
    "              'Downloads/dataset/Arrest/Arrest011_x264.mp4']  # Replace with actual paths\n",
    "\n",
    "labels = [0, 0, 0,0, 0, 1,1,1, 1, 1, 1, 2, 2,2,2]  # Example: burglary (0), abuse (1), arrest (2)\n",
    "\n",
    "# Preprocess Data\n",
    "X, y = preprocess_data(video_paths, labels, start_frame=200, end_frame=230)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build Models\n",
    "cnn = build_cnn()\n",
    "model = build_spatio_temporal_model(cnn, FRAME_COUNT, NUM_CLASSES)\n",
    "\n",
    "# Train the Model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the Model\n",
    "model.save('activity_recognition_model1.keras')\n",
    "\n",
    "# Real-Time Detection\n",
    "def predict_activity(model, video_path):\n",
    "    \"\"\"Predict the activity in a video.\"\"\"\n",
    "    frames = extract_frames(video_path)\n",
    "    frames = frames[:FRAME_COUNT]\n",
    "    frames = np.expand_dims(frames, axis=0)\n",
    "    prediction = model.predict(frames)\n",
    "    return np.argmax(prediction)\n",
    "\n",
    "# Test Prediction\n",
    "test_video = 'Downloads/dataset/Burglary/Burglary059_x264.mp4'  # Replace with the path to your test video\n",
    "result = predict_activity(model, test_video)\n",
    "activity_labels = ['Burglary', 'Abuse', 'Arrest']\n",
    "print(f\"Predicted activity: {activity_labels[result]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7494b80-205a-40cd-a824-43ebce80cb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61440</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,864,448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m3\u001b[0m)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2048\u001b[0m)       │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61440\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m7,864,448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m387\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">94,251,403</span> (359.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m94,251,403\u001b[0m (359.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,399,427</span> (119.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,399,427\u001b[0m (119.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,120</span> (207.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m53,120\u001b[0m (207.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,798,856</span> (239.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m62,798,856\u001b[0m (239.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e738c4-418f-493a-888c-aabe1ad00e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22s/step - accuracy: 0.3333 - loss: 59.9799\n",
      "Test Accuracy: 33.33%\n",
      "Predicted activity: Abuse\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"activity_recognition_model1.keras\"  \n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Test video path\n",
    "test_video = \"Downloads/dataset/Abuse/Abuse016_x264.mp4\" \n",
    "\n",
    "# Predict activity using the trained model\n",
    "result = predict_activity(model, test_video)\n",
    "# Evaluate the Model on Test Data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the Accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "# Activity labels\n",
    "activity_labels = ['Burglary', 'Abuse', 'Arrest']\n",
    "print(f\"Predicted activity: {activity_labels[result]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69774db6-fcae-4857-9add-b9fea61b157e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Video classification using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e591cf-87ea-4be2-9327-587aa193d141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 50s/step - accuracy: 0.2917 - loss: 63.7929 - val_accuracy: 0.3333 - val_loss: 82.7560\n",
      "Epoch 2/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 60s/step - accuracy: 0.2500 - loss: 58.0696 - val_accuracy: 0.3333 - val_loss: 13.3908\n",
      "Epoch 3/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 54s/step - accuracy: 0.2083 - loss: 45.7541 - val_accuracy: 0.3333 - val_loss: 1.3152\n",
      "Epoch 4/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 56s/step - accuracy: 0.4028 - loss: 1.2288 - val_accuracy: 0.3333 - val_loss: 1.1029\n",
      "Epoch 5/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 55s/step - accuracy: 0.4861 - loss: 2.5527 - val_accuracy: 0.3333 - val_loss: 1.3305\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "Predicted activity: Burglary\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, TimeDistributed, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "# Global Configurations\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 3  \n",
    "\n",
    "# Dataset Preparation\n",
    "def extract_frames(video_path, output_size=IMAGE_SIZE):\n",
    "    \"\"\"Extract frames from a video and resize them.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def preprocess_data(video_paths, labels, start_frame=150, end_frame=180):\n",
    "    \"\"\"Preprocess videos into fixed frame count sequences from a specified range.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        frames = extract_frames(video_path)\n",
    "        if len(frames) > end_frame:  # Ensure there are enough frames\n",
    "            X.append(frames[start_frame:end_frame])  # Take frames from start_frame to end_frame\n",
    "            y.append(labels[i])\n",
    "    return np.array(X), to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "def build_cnn():\n",
    "    \"\"\"Build a CNN model for spatial feature extraction using VGG16.\"\"\"\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Combined Spatio-Temporal Model without RNN\n",
    "def build_spatio_temporal_model(cnn, frame_count, num_classes):\n",
    "    \"\"\"Combine CNN for spatio-temporal analysis.\"\"\"\n",
    "    video_input = Input(shape=(frame_count, 224, 224, 3))\n",
    "    cnn_output = TimeDistributed(cnn)(video_input)\n",
    "    \n",
    "    # Flatten the output and add Dense layers for classification\n",
    "    x = Flatten()(cnn_output)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=video_input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example Dataset\n",
    "video_paths = ['Downloads/dataset/Burglary/Burglary059_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary009_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary062_x264.mp4', \n",
    "               'Downloads/dataset/Burglary/Burglary083_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary089_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse016_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse018_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse022_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse032_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse041_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse044_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest030_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest033_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest015_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest011_x264.mp4']  # Replace with actual paths\n",
    "\n",
    "labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2]  # Example: burglary (0), abuse (1), arrest (2)\n",
    "\n",
    "# Preprocess Data\n",
    "X, y = preprocess_data(video_paths, labels, start_frame=150, end_frame=180)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build Models\n",
    "cnn = build_cnn()\n",
    "model = build_spatio_temporal_model(cnn, FRAME_COUNT, NUM_CLASSES)\n",
    "\n",
    "# Train the Model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the Model\n",
    "model.save('activity_recognition_model_vgg16.keras')\n",
    "\n",
    "# Real-Time Detection\n",
    "def predict_activity(model, video_path):\n",
    "    \"\"\"Predict the activity in a video.\"\"\"\n",
    "    frames = extract_frames(video_path)\n",
    "    frames = frames[:FRAME_COUNT]\n",
    "    frames = np.expand_dims(frames, axis=0)\n",
    "    prediction = model.predict(frames)\n",
    "    return np.argmax(prediction)\n",
    "\n",
    "# Test Prediction\n",
    "test_video = 'Downloads/dataset/Burglary/Burglary059_x264.mp4'  # Replace with the path to your test video\n",
    "result = predict_activity(model, test_video)\n",
    "activity_labels = ['Burglary', 'Abuse', 'Arrest']\n",
    "print(f\"Predicted activity: {activity_labels[result]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d50ecc80-314c-4ab6-8eef-2c8334e78902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.3333 - loss: 1.3305\n",
      "Test Accuracy: 33.33%\n",
      "Predicted activity: Burglary\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"activity_recognition_model_vgg16.keras\"  \n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Test video path\n",
    "test_video = \"Downloads/dataset/Burglary/Burglary059_x264.mp4\" \n",
    "\n",
    "# Predict activity using the trained model\n",
    "result = predict_activity(model, test_video)\n",
    "# Evaluate the Model on Test Data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the Accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "# Activity labels\n",
    "activity_labels = ['Burglary', 'Abuse', 'Arrest']\n",
    "print(f\"Predicted activity: {activity_labels[result]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501ab75-86e1-4bd2-8566-a7a74dfbb8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a1cecb-17d0-4be5-8990-cc3b3aff35ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 366s/step - accuracy: 1.0000 - loss: 1.0195 - val_accuracy: 1.0000 - val_loss: 0.4508\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - accuracy: 1.0000 - loss: 0.4450 - val_accuracy: 1.0000 - val_loss: 0.2578\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 26s/step - accuracy: 1.0000 - loss: 0.2564 - val_accuracy: 1.0000 - val_loss: 0.1467\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 23s/step - accuracy: 1.0000 - loss: 0.1456 - val_accuracy: 1.0000 - val_loss: 0.0863\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 28s/step - accuracy: 1.0000 - loss: 0.0865 - val_accuracy: 1.0000 - val_loss: 0.0521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m15512841536\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=tf.Tensor(shape=(1, 30, 2048), dtype=float32)\\n  • training=False\\n  • mask=None'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 149\u001b[0m\n\u001b[1;32m    147\u001b[0m visualize_feature_maps(cnn, sample_video)\n\u001b[1;32m    148\u001b[0m cnn_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([cnn\u001b[38;5;241m.\u001b[39mpredict(video) \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m X_train])\n\u001b[0;32m--> 149\u001b[0m \u001b[43mvisualize_rnn_hidden_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m sample_frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(extract_frames(sample_video)[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    151\u001b[0m generate_cam(cnn, sample_frame)\n",
      "Cell \u001b[0;32mIn[8], line 99\u001b[0m, in \u001b[0;36mvisualize_rnn_hidden_states\u001b[0;34m(rnn_model, cnn_features)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Visualize hidden states of LSTM layers.\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m rnn_intermediate \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39mrnn_model\u001b[38;5;241m.\u001b[39minput, outputs\u001b[38;5;241m=\u001b[39mrnn_model\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moutput)  \u001b[38;5;66;03m# First LSTM layer\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_intermediate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m    102\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(hidden_states[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Plot hidden states for the first video\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/keras/src/ops/function.py:179\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    177\u001b[0m output_tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs:\n\u001b[0;32m--> 179\u001b[0m     output_tensors\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtensor_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mpack_sequence_as(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_struct, output_tensors)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m15512841536\\x1b[0m\\n\\nArguments received by Functional.call():\\n  • inputs=tf.Tensor(shape=(1, 30, 2048), dtype=float32)\\n  • training=False\\n  • mask=None'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Global Configurations\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 3  \n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "\n",
    "# Dataset Preparation\n",
    "def extract_frames(video_path, output_size=IMAGE_SIZE):\n",
    "    \"\"\"Extract frames from a video and resize them.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def preprocess_data(video_paths, labels, frame_count=FRAME_COUNT):\n",
    "    \"\"\"Preprocess videos into fixed frame count sequences.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        frames = extract_frames(video_path)\n",
    "        if len(frames) >= frame_count:\n",
    "            X.append(frames[:frame_count])\n",
    "            y.append(labels[i])\n",
    "    return np.array(X), to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "def build_cnn():\n",
    "    \"\"\"Build a CNN model for spatial feature extraction.\"\"\"\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# RNN Model for Temporal Analysis\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    \"\"\"Build an RNN for temporal feature analysis.\"\"\"\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),  \n",
    "        LSTM(256, return_sequences=True, name='lstm_1'),\n",
    "        LSTM(128, return_sequences=True, name='lstm_2'),\n",
    "        LSTM(64, return_sequences=False, name='lstm_3'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Combined Spatio-Temporal Model\n",
    "def build_spatio_temporal_model(cnn, rnn, frame_count, num_classes):\n",
    "    \"\"\"Combine CNN and RNN for spatio-temporal analysis.\"\"\"\n",
    "    video_input = Input(shape=(frame_count, 224, 224, 3))\n",
    "    cnn_output = TimeDistributed(cnn)(video_input)\n",
    "    rnn_output = rnn(cnn_output)\n",
    "    model = Model(inputs=video_input, outputs=rnn_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Feature Map Visualization for CNN\n",
    "def visualize_feature_maps(cnn_model, video_path, layer_name='conv5_block3_out'):\n",
    "    \"\"\"Visualize feature maps from a specific CNN layer.\"\"\"\n",
    "    frames = extract_frames(video_path)[:FRAME_COUNT]\n",
    "    frames = np.expand_dims(frames[0], axis=0)  # Select the first frame\n",
    "    \n",
    "    intermediate_model = Model(inputs=cnn_model.input, outputs=cnn_model.get_layer(layer_name).output)\n",
    "    feature_maps = intermediate_model.predict(frames)\n",
    "    \n",
    "    num_features = feature_maps.shape[-1]  # Number of filters\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(min(num_features, 16)):  # Display first 16 feature maps\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Feature Maps from Layer: {layer_name}\")\n",
    "    plt.show()\n",
    "\n",
    "# RNN Hidden State Evolution\n",
    "def visualize_rnn_hidden_states(rnn_model, cnn_features):\n",
    "    \"\"\"Visualize hidden states of LSTM layers.\"\"\"\n",
    "    rnn_intermediate = Model(inputs=rnn_model.input, outputs=rnn_model.get_layer('lstm_1').output)  # First LSTM layer\n",
    "    hidden_states = rnn_intermediate.predict(cnn_features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(hidden_states[0])  # Plot hidden states for the first video\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Hidden State Activation\")\n",
    "    plt.title(\"LSTM Hidden State Evolution\")\n",
    "    plt.show()\n",
    "\n",
    "# Class Activation Maps (CAM)\n",
    "def generate_cam(model, img_array, layer_name='conv5_block3_out'):\n",
    "    \"\"\"Generate Class Activation Map (CAM).\"\"\"\n",
    "    class_weights = model.layers[-1].get_weights()[0]\n",
    "    final_conv_layer = model.get_layer(layer_name)\n",
    "    grad_model = Model(inputs=model.input, outputs=[final_conv_layer.output, model.output])\n",
    "    conv_output, predictions = grad_model.predict(img_array)\n",
    "    \n",
    "    class_activation = np.dot(conv_output[0], class_weights[:, np.argmax(predictions)])\n",
    "    \n",
    "    plt.imshow(class_activation, cmap='jet')\n",
    "    plt.title(\"Class Activation Map\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example Dataset\n",
    "video_paths = ['Downloads/dataset/Burglary/Burglary059_x264.mp4']  \n",
    "labels = [0]  \n",
    "\n",
    "# Preprocess Data\n",
    "X, y = preprocess_data(video_paths, labels, frame_count=FRAME_COUNT)\n",
    "if len(X) > 1:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = X, X, y, y  # Use all data for training\n",
    "\n",
    "# Build Models\n",
    "cnn = build_cnn()\n",
    "rnn = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "model = build_spatio_temporal_model(cnn, rnn, FRAME_COUNT, NUM_CLASSES)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the Model\n",
    "model.save('activity_recognition_model.keras')\n",
    "\n",
    "# Visualizations\n",
    "sample_video = 'Downloads/dataset/Burglary/Burglary059_x264.mp4'\n",
    "visualize_feature_maps(cnn, sample_video)\n",
    "cnn_features = np.array([cnn.predict(video) for video in X_train])\n",
    "visualize_rnn_hidden_states(rnn, cnn_features)\n",
    "sample_frame = np.expand_dims(extract_frames(sample_video)[0], axis=0)\n",
    "generate_cam(cnn, sample_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fcd3d6-23ed-4e89-9420-7b75ae991d40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1c8bf-5b1e-499b-9a4c-c74bc6be5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and preprocessing video data...\n",
      "Total video sequences generated: 328\n",
      "Starting training...\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Global Configurations\n",
    "FRAME_COUNT = 30\n",
    "STRIDE = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 3\n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "\n",
    "# Dataset Preparation\n",
    "def extract_frames(video_path, output_size=IMAGE_SIZE):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"[ERROR] Cannot open video file: {video_path}\")\n",
    "        return []\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def preprocess_data_sliding_window(video_paths, labels, frame_count=FRAME_COUNT, stride=STRIDE):\n",
    "    X, y = [], []\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        frames = extract_frames(video_path)\n",
    "        if len(frames) < frame_count:\n",
    "            print(f\"[WARNING] Skipping {video_path} — too short or unreadable.\")\n",
    "            continue\n",
    "        for start in range(0, len(frames) - frame_count + 1, stride):\n",
    "            clip = frames[start:start + frame_count]\n",
    "            X.append(clip)\n",
    "            y.append(labels[i])\n",
    "    return np.array(X), to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "def build_cnn():\n",
    "    \"\"\"Build a CNN model for spatial feature extraction.\"\"\"\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# RNN Model for Temporal Analysis\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    \"\"\"Build an RNN for temporal feature analysis.\"\"\"\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        LSTM(256, return_sequences=True),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Combined Spatio-Temporal Model\n",
    "def build_spatio_temporal_model(cnn, rnn, frame_count, num_classes):\n",
    "    \"\"\"Combine CNN and RNN for spatio-temporal analysis.\"\"\"\n",
    "    video_input = Input(shape=(frame_count, 224, 224, 3))\n",
    "    cnn_output = TimeDistributed(cnn)(video_input)\n",
    "    rnn_output = rnn(cnn_output)\n",
    "    model = Model(inputs=video_input, outputs=rnn_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example Dataset\n",
    "video_paths = [\n",
    "    'Downloads/dataset/Burglary/Burglary009_x264.mp4',\n",
    "    'Downloads/dataset/Abuse/Abuse032_x264.mp4',\n",
    "    'Downloads/dataset/Arrest/Arrest015_x264.mp4',\n",
    "    'Downloads/dataset/Arrest/Arrest042_x264.mp4'\n",
    "]\n",
    "labels = [0, 1, 2, 2]\n",
    "\n",
    "# Preprocess Data using sliding window\n",
    "print(\"Extracting and preprocessing video data...\")\n",
    "X, y = preprocess_data_sliding_window(video_paths, labels, frame_count=FRAME_COUNT, stride=STRIDE)\n",
    "print(f\"Total video sequences generated: {X.shape[0]}\")\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build Models\n",
    "cnn = build_cnn()\n",
    "rnn = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "model = build_spatio_temporal_model(cnn, rnn, FRAME_COUNT, NUM_CLASSES)\n",
    "\n",
    "# Train the Model\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(X_train, y_train, epochs=2 , batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the Model\n",
    "model.save('activity_recognition_model_fullvideo.keras')\n",
    "print(\"Model saved to 'activity_recognition_model_fullvideo.keras'\")\n",
    "\n",
    "# Plot Loss and Accuracy per Class\n",
    "def plot_training_history_per_class(history):\n",
    "    \"\"\"Plot the training loss and accuracy for each class.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i, class_name in enumerate(CLASS_NAMES):\n",
    "        plt.subplot(3, 2, i + 1)\n",
    "        plt.plot(history.history['loss'], label=f'{class_name} Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label=f'{class_name} Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss for {class_name}')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3, 2, i + 4)\n",
    "        plt.plot(history.history['accuracy'], label=f'{class_name} Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label=f'{class_name} Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Accuracy for {class_name}')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history_per_class(history)\n",
    "\n",
    "# Real-Time Single Video Prediction\n",
    "def predict_activity(model, video_path):\n",
    "    \"\"\"Predict the activity from the first 30 frames of a video (for quick test).\"\"\"\n",
    "    frames = extract_frames(video_path)\n",
    "    if len(frames) < FRAME_COUNT:\n",
    "        print(\"Video too short for prediction.\")\n",
    "        return None\n",
    "    frames = frames[:FRAME_COUNT]\n",
    "    frames = np.expand_dims(frames, axis=0)\n",
    "    prediction = model.predict(frames)\n",
    "    return np.argmax(prediction)\n",
    "\n",
    "# Test Prediction\n",
    "test_video = 'Downloads/dataset/Burglary/Burglary059_x264.mp4'\n",
    "result = predict_activity(model, test_video)\n",
    "if result is not None:\n",
    "    print(f\"Predicted activity: {CLASS_NAMES[result]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42308000-d622-4323-b6b3-ac9e35dd23d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33s/step - accuracy: 1.0000 - loss: 0.0034\n",
      "Test Accuracy: 100.00%\n",
      "Predicted activity: Abuse\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"activity_recognition_model_fullvideo.keras\"  \n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Test video path\n",
    "test_video = \"Downloads/dataset/Burglary/Burglary062_x264.mp4\" \n",
    "\n",
    "# Predict activity using the trained model\n",
    "result = predict_activity(model, test_video)\n",
    "# Evaluate the Model on Test Data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the Accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "# Activity labels\n",
    "activity_labels = ['Burglary', 'Abuse', 'Arrest']\n",
    "print(f\"Predicted activity: {activity_labels[result]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e4de7-518e-46e6-bfa0-74183b698701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
