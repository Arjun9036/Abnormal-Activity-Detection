{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfbc97d-c066-4d2e-8179-57b36a7fdba9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d728d5f6-fc6a-44c7-94f5-070bb3b4214b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T17:17:08.839513Z",
     "start_time": "2025-02-16T17:17:08.766548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 211s/step - accuracy: 0.2500 - loss: 1.1915 - val_accuracy: 0.0000e+00 - val_loss: 1.2184\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 143s/step - accuracy: 0.8750 - loss: 0.6989 - val_accuracy: 0.0000e+00 - val_loss: 1.2698\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 126s/step - accuracy: 1.0000 - loss: 0.3766 - val_accuracy: 0.0000e+00 - val_loss: 1.5217\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 132s/step - accuracy: 1.0000 - loss: 0.1900 - val_accuracy: 0.0000e+00 - val_loss: 1.9230\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 138s/step - accuracy: 1.0000 - loss: 0.1049 - val_accuracy: 0.0000e+00 - val_loss: 2.2826\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13s/step\n",
      "Predicted activity: Arrest\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Global Configurations\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 3  \n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "\n",
    "# Dataset Preparation\n",
    "def extract_frames(video_path, output_size=IMAGE_SIZE):\n",
    "    \"\"\"Extract frames from a video and resize them.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def preprocess_data(video_paths, labels, frame_count=FRAME_COUNT):\n",
    "    \"\"\"Preprocess videos into fixed frame count sequences.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        frames = extract_frames(video_path)\n",
    "        if len(frames) >= frame_count:\n",
    "            X.append(frames[:frame_count])\n",
    "            y.append(labels[i])\n",
    "    return np.array(X), to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "def build_cnn():\n",
    "    \"\"\"Build a CNN model for spatial feature extraction.\"\"\"\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# RNN Model for Temporal Analysis\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    \"\"\"Build an RNN for temporal feature analysis.\"\"\"\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),  \n",
    "        LSTM(256, return_sequences=True),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Combined Spatio-Temporal Model\n",
    "def build_spatio_temporal_model(cnn, rnn, frame_count, num_classes):\n",
    "    \"\"\"Combine CNN and RNN for spatio-temporal analysis.\"\"\"\n",
    "    video_input = Input(shape=(frame_count, 224, 224, 3))\n",
    "    cnn_output = TimeDistributed(cnn)(video_input)\n",
    "    ##print cnn_output\n",
    "    rnn_output = rnn(cnn_output)\n",
    "    ##print rnn o/p\n",
    "    model = Model(inputs=video_input, outputs=rnn_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example Dataset\n",
    "video_paths = ['Downloads/dataset/Burglary/Burglary009_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary059_x264.mp4', \n",
    "               'Downloads/dataset/Burglary/Burglary098_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse016_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse018_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse019_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse041_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest015_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest011_x264.mp4',\n",
    "              'Downloads/dataset/Arrest/Arrest042_x264.mp4'\n",
    "              ]  \n",
    "labels = [0, 0, 0, 1, 1, 1, 1, 2, 2, 2]  \n",
    "\n",
    "# Preprocess Data\n",
    "X, y = preprocess_data(video_paths, labels, frame_count=FRAME_COUNT)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build Models\n",
    "cnn = build_cnn()\n",
    "rnn = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "model = build_spatio_temporal_model(cnn, rnn, FRAME_COUNT, NUM_CLASSES)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the Model\n",
    "model.save('activity_recognition_model.keras')\n",
    "\n",
    "# Real-Time Detection\n",
    "def predict_activity(model, video_path):\n",
    "    \"\"\"Predict the activity in a video.\"\"\"\n",
    "    frames = extract_frames(video_path)\n",
    "    frames = frames[:FRAME_COUNT]\n",
    "    frames = np.expand_dims(frames, axis=0)\n",
    "    prediction = model.predict(frames)\n",
    "    return np.argmax(prediction)\n",
    "\n",
    "# Test Prediction\n",
    "test_video = 'Downloads/dataset/Arrest/Arrest042_x264.mp4'\n",
    "result = predict_activity(model, test_video)\n",
    "print(f\"Predicted activity: {CLASS_NAMES[result]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d758ff-9f89-40b4-a819-fb5dd5ce965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flask\n",
      "  Downloading flask-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Requirement already satisfied: jinja2>=3.1.2 in ./myenv/lib/python3.10/site-packages (from flask) (3.1.5)\n",
      "Collecting itsdangerous>=2.2.0\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in ./myenv/lib/python3.10/site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: click>=8.1.3 in ./myenv/lib/python3.10/site-packages (from flask) (8.1.8)\n",
      "Collecting blinker>=1.9.0\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in ./myenv/lib/python3.10/site-packages (from flask) (3.0.2)\n",
      "Installing collected packages: itsdangerous, blinker, flask\n",
      "Successfully installed blinker-1.9.0 flask-3.1.1 itsdangerous-2.2.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/arjungoyal/myenv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bac1a71-caf6-4133-844e-66a462fbd790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arjungoyal/myenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "import logging\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from flask import Flask, render_template_string\n",
    "from PyQt5.QtWidgets import QApplication, QLabel, QPushButton, QWidget, QVBoxLayout\n",
    "from PyQt5.QtGui import QImage, QPixmap\n",
    "from PyQt5.QtCore import QTimer\n",
    "from tensorflow.keras.models import load_model\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# --------- Logging Setup ----------\n",
    "logging.basicConfig(filename=\"alerts.log\", level=logging.INFO, \n",
    "                    format='%(asctime)s - %(message)s')\n",
    "\n",
    "# --------- Alert System ----------\n",
    "def send_email_alert(subject, body):\n",
    "    try:\n",
    "        sender = \"youremail@example.com\"\n",
    "        receiver = \"receiver@example.com\"\n",
    "        password = \"your_app_password\"\n",
    "\n",
    "        msg = MIMEText(body)\n",
    "        msg['Subject'] = subject\n",
    "        msg['From'] = sender\n",
    "        msg['To'] = receiver\n",
    "\n",
    "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
    "            server.login(sender, password)\n",
    "            server.send_message(msg)\n",
    "        logging.info(\"Email alert sent.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to send alert email: {e}\")\n",
    "\n",
    "\n",
    "# --------- Preprocessing Function ----------\n",
    "def preprocess_frame(frame, target_size=(224, 224)):\n",
    "    frame_resized = cv2.resize(frame, target_size)\n",
    "    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "    frame_normalized = frame_rgb.astype(\"float32\") / 255.0\n",
    "    return frame_normalized\n",
    "\n",
    "\n",
    "# --------- Real-Time Activity Detector ----------\n",
    "class RealTimeActivityDetector:\n",
    "    def __init__(self, model_path, sequence_length=30):\n",
    "        self.model = load_model(model_path)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.frame_buffer = deque(maxlen=sequence_length)\n",
    "        self.labels = ['burglary', 'abuse', 'arrest']  # update with your model's classes\n",
    "\n",
    "    def predict(self, frame):\n",
    "        preprocessed = preprocess_frame(frame)\n",
    "        self.frame_buffer.append(preprocessed)\n",
    "\n",
    "        if len(self.frame_buffer) == self.sequence_length:\n",
    "            sequence = np.expand_dims(np.array(self.frame_buffer), axis=0)\n",
    "            predictions = self.model.predict(sequence, verbose=0)[0]\n",
    "            label_idx = np.argmax(predictions)\n",
    "            confidence = predictions[label_idx]\n",
    "            return self.labels[label_idx], confidence\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# --------- PyQt GUI ----------\n",
    "class SurveillanceApp(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Real-Time Activity Detection\")\n",
    "        self.setGeometry(100, 100, 800, 600)\n",
    "\n",
    "        self.video_label = QLabel(\"Video Feed\")\n",
    "        self.start_btn = QPushButton(\"Start Detection\")\n",
    "        self.stop_btn = QPushButton(\"Stop Detection\")\n",
    "\n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.video_label)\n",
    "        layout.addWidget(self.start_btn)\n",
    "        layout.addWidget(self.stop_btn)\n",
    "        self.setLayout(layout)\n",
    "\n",
    "        self.cap = None\n",
    "        self.timer = QTimer()\n",
    "        self.timer.timeout.connect(self.update_frame)\n",
    "        self.detector = RealTimeActivityDetector(\"activity_recognition_model.keras\")\n",
    "        self.last_alert_time = datetime.min\n",
    "\n",
    "        self.start_btn.clicked.connect(self.start_camera)\n",
    "        self.stop_btn.clicked.connect(self.stop_camera)\n",
    "\n",
    "    def start_camera(self):\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.timer.start(30)  # roughly 30 FPS\n",
    "\n",
    "    def update_frame(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            return\n",
    "\n",
    "        label, confidence = self.detector.predict(frame)\n",
    "        if label and confidence > 0.8:\n",
    "            cv2.putText(frame, f\"{label.upper()} ({confidence*100:.1f}%)\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            # Alert every 10 sec max\n",
    "            if (datetime.now() - self.last_alert_time).seconds > 10:\n",
    "                logging.info(f\"Alert: {label} detected with confidence {confidence:.2f}\")\n",
    "                send_email_alert(\"ALERT: Suspicious Activity\", f\"{label} detected with {confidence*100:.2f}% confidence\")\n",
    "                self.last_alert_time = datetime.now()\n",
    "        else:\n",
    "            cv2.putText(frame, \"Normal\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        h, w, ch = rgb_frame.shape\n",
    "        qt_image = QImage(rgb_frame.data, w, h * ch, QImage.Format_RGB888)\n",
    "        self.video_label.setPixmap(QPixmap.fromImage(qt_image))\n",
    "\n",
    "    def stop_camera(self):\n",
    "        self.timer.stop()\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "        self.video_label.clear()\n",
    "\n",
    "\n",
    "# --------- Flask App for LAN Access ----------\n",
    "flask_app = Flask(__name__)\n",
    "\n",
    "@flask_app.route('/')\n",
    "def index():\n",
    "    return render_template_string(\"<h2>Real-Time Activity Detection Running</h2>\")\n",
    "\n",
    "\n",
    "def run_flask():\n",
    "    try:\n",
    "        flask_app.run(host='0.0.0.0', port=5050)  # safer default port\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Flask server error: {e}\")\n",
    "\n",
    "\n",
    "# --------- Main Entry ----------\n",
    "if __name__ == \"__main__\":\n",
    "    flask_thread = threading.Thread(target=run_flask, daemon=True)\n",
    "    flask_thread.start()\n",
    "\n",
    "    qt_app = QApplication(sys.argv)\n",
    "    window = SurveillanceApp()\n",
    "    window.show()\n",
    "    sys.exit(qt_app.exec_())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783531a-4306-4369-9c63-0d40d1c661ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5bdf74c-9df5-4dd2-a06e-2e1f3fd42200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\n",
      "âœ… CNN Output shape: (1, 30, 2048) (frame_count x feature_vector_size)\n",
      "CNN Output Vector (1st frame):\n",
      " [0.00599981 2.078606   0.45295173 ... 1.1780607  0.24603099 0.9764378 ]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\n",
      "âœ… RNN Output shape: (1, 3) (1 x num_classes)\n",
      "RNN Output Vector:\n",
      " [[0.27041426 0.38492414 0.34466153]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step\n",
      "\n",
      "ğŸ¯ Final Model Prediction: Burglary\n",
      "Final Model Output Vector:\n",
      " [[0.8912993  0.08720166 0.02149893]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import TimeDistributed, Input\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# Constants\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# --- Load and Build Components Again (Same Architecture Required) ---\n",
    "\n",
    "# Load trained full model\n",
    "full_model = load_model('activity_recognition_model.keras')\n",
    "\n",
    "# Rebuild CNN\n",
    "def build_cnn():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "cnn_model = build_cnn()\n",
    "\n",
    "# Rebuild RNN\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        LSTM(256, return_sequences=True),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "rnn_model = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "\n",
    "# --- Frame Extraction ---\n",
    "def extract_frames(video_path, output_size=(224, 224), frame_count=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "        if len(frames) == frame_count:\n",
    "            break\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "# --- Load test video ---\n",
    "video_path = 'Downloads/dataset/Burglary/Burglary059_x264.mp4'\n",
    "frames = extract_frames(video_path)\n",
    "frames = frames[:FRAME_COUNT]\n",
    "frames_batch = np.expand_dims(frames, axis=0)  # (1, 30, 224, 224, 3)\n",
    "\n",
    "# --- Get CNN Output ---\n",
    "cnn_features = np.array([cnn_model.predict(frames_batch[0])])  # shape: (30, 2048)\n",
    "print(f\"\\nâœ… CNN Output shape: {cnn_features.shape} (frame_count x feature_vector_size)\")\n",
    "print(\"CNN Output Vector (1st frame):\\n\", cnn_features[0][0])\n",
    "\n",
    "# --- Get RNN Output ---\n",
    "rnn_input = np.expand_dims(cnn_features[0], axis=0)  # (1, 30, 2048)\n",
    "rnn_output = rnn_model.predict(rnn_input)\n",
    "print(f\"\\nâœ… RNN Output shape: {rnn_output.shape} (1 x num_classes)\")\n",
    "print(\"RNN Output Vector:\\n\", rnn_output)\n",
    "\n",
    "# --- Final Model Prediction ---\n",
    "prediction = full_model.predict(frames_batch)\n",
    "predicted_class = np.argmax(prediction)\n",
    "print(f\"\\nğŸ¯ Final Model Prediction: {CLASS_NAMES[predicted_class]}\")\n",
    "print(\"Final Model Output Vector:\\n\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb321f-13cd-4495-a230-374497d77cf1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CNN & RNN outputs CSV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d75a0c3b-82bb-461f-8ec4-998ec1b10f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary089_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary010_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary083_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary059_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary098_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary062_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary009_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting036_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting037_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting030_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting005_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting013_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting048_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting022_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest015_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest030_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest029_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest042_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest043_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest033_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest011_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_606_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_781_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_189_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_941_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_365_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_696_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_010_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse019_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse018_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse041_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse032_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse044_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse016_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse022_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault002_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault018_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault051_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault029_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault011_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault045_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault033_x264.mp4\n",
      "\n",
      "âœ… All predictions logged to: activity_predictions_log.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Constants\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# Paths\n",
    "VIDEO_DIR = 'Downloads/dataset'  # Root dataset directory\n",
    "OUTPUT_CSV = 'activity_predictions_log.csv'\n",
    "\n",
    "# Load trained full model\n",
    "full_model = load_model('activity_recognition_model.keras')\n",
    "\n",
    "# Rebuild CNN\n",
    "def build_cnn():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "cnn_model = build_cnn()\n",
    "\n",
    "# Rebuild RNN\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(256, return_sequences=True),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "rnn_model = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "\n",
    "# Extract frames from video\n",
    "def extract_frames(video_path, output_size=(224, 224), frame_count=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "        if len(frames) == frame_count:\n",
    "            break\n",
    "    cap.release()\n",
    "    if len(frames) < frame_count:\n",
    "        print(f\"âš ï¸ Skipping {video_path}: Not enough frames ({len(frames)})\")\n",
    "        return None\n",
    "    return np.array(frames)\n",
    "\n",
    "# Walk through videos and make predictions\n",
    "def process_videos_and_log(video_dir, output_csv):\n",
    "    results = []\n",
    "\n",
    "    for root, _, files in os.walk(video_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                video_path = os.path.join(root, file)\n",
    "                print(f\"\\nğŸ“¹ Processing: {video_path}\")\n",
    "                \n",
    "                frames = extract_frames(video_path)\n",
    "                if frames is None:\n",
    "                    continue\n",
    "                \n",
    "                frames_batch = np.expand_dims(frames, axis=0)  # (1, 30, 224, 224, 3)\n",
    "\n",
    "                # CNN Output\n",
    "                cnn_features = np.array([cnn_model.predict(frames_batch[0], verbose=0)])  # (1, 30, 2048)\n",
    "                \n",
    "                # RNN Output\n",
    "                rnn_input = np.expand_dims(cnn_features[0], axis=0)  # (1, 30, 2048)\n",
    "                rnn_output = rnn_model.predict(rnn_input, verbose=0)  # (1, num_classes)\n",
    "\n",
    "                # Final Model Output\n",
    "                final_output = full_model.predict(frames_batch, verbose=0)\n",
    "                predicted_class = np.argmax(final_output)\n",
    "\n",
    "                # Save record\n",
    "                results.append({\n",
    "                    \"Video Name\": file,\n",
    "                    \"Predicted Class\": CLASS_NAMES[predicted_class],\n",
    "                    \"CNN Vector (1st Frame)\": cnn_features[0][0][:5].tolist(),  # First 5 values for brevity\n",
    "                    \"RNN Output\": rnn_output[0].tolist(),\n",
    "                    \"Final Output\": final_output[0].tolist()\n",
    "                })\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(output_csv, mode='w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=results[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in results:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"\\nâœ… All predictions logged to: {output_csv}\")\n",
    "\n",
    "# Run it\n",
    "process_videos_and_log(VIDEO_DIR, OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c41338a4-8840-4a9e-9064-000256318cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary089_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary010_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary083_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary059_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary098_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary062_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Burglary/Burglary009_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting036_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting037_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting030_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting005_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting013_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting048_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Fighting/Fighting022_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest015_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest030_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest029_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest042_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest043_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest033_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Arrest/Arrest011_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_606_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_781_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_189_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_941_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_365_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_696_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/normal/Normal_Videos_010_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse019_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse018_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse041_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse032_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse044_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse016_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Abuse/Abuse022_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault002_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault018_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault051_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault029_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault011_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault045_x264.mp4\n",
      "\n",
      "ğŸ“¹ Processing: Downloads/dataset/Assault/Assault033_x264.mp4\n",
      "\n",
      "âœ… All predictions logged with CNN data from 5 frames â†’ activity_predictions_log_detailed1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# Constants\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# Paths\n",
    "VIDEO_DIR = 'Downloads/dataset'  # Root dataset directory\n",
    "OUTPUT_CSV = 'activity_predictions_log_detailed1.csv'\n",
    "\n",
    "# Load trained full model\n",
    "full_model = load_model('activity_recognition_model.keras')\n",
    "\n",
    "# Rebuild CNN\n",
    "def build_cnn():\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "cnn_model = build_cnn()\n",
    "\n",
    "# Rebuild RNN\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(256, return_sequences=True),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "rnn_model = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "\n",
    "# Extract frames from video\n",
    "def extract_frames(video_path, output_size=(224, 224), frame_count=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "        if len(frames) == frame_count:\n",
    "            break\n",
    "    cap.release()\n",
    "    if len(frames) < frame_count:\n",
    "        print(f\"âš ï¸ Skipping {video_path}: Not enough frames ({len(frames)})\")\n",
    "        return None\n",
    "    return np.array(frames)\n",
    "\n",
    "# Walk through videos and make predictions\n",
    "def process_videos_and_log(video_dir, output_csv):\n",
    "    results = []\n",
    "\n",
    "    for root, _, files in os.walk(video_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                video_path = os.path.join(root, file)\n",
    "                print(f\"\\nğŸ“¹ Processing: {video_path}\")\n",
    "                \n",
    "                frames = extract_frames(video_path)\n",
    "                if frames is None:\n",
    "                    continue\n",
    "                \n",
    "                frames_batch = np.expand_dims(frames, axis=0)  # (1, 30, 224, 224, 3)\n",
    "\n",
    "                # CNN Output\n",
    "                cnn_features = np.array([cnn_model.predict(frames_batch[0], verbose=0)])  # (1, 30, 2048)\n",
    "\n",
    "                # RNN Output\n",
    "                rnn_input = np.expand_dims(cnn_features[0], axis=0)  # (1, 30, 2048)\n",
    "                rnn_output = rnn_model.predict(rnn_input, verbose=0)  # (1, num_classes)\n",
    "\n",
    "                # Final Model Output\n",
    "                final_output = full_model.predict(frames_batch, verbose=0)\n",
    "                predicted_class = np.argmax(final_output)\n",
    "\n",
    "                # Prepare 5 CNN vectors for CSV (reduce size for readability)\n",
    "                cnn_vectors = cnn_features[0][:5]  # shape: (5, 2048)\n",
    "                cnn_vectors_flat = []\n",
    "                for i in range(5):\n",
    "                    # Add first 10 features from each frame for brevity\n",
    "                    cnn_vectors_flat.extend(cnn_vectors[i][:10].tolist())\n",
    "\n",
    "                results.append({\n",
    "                    \"Video Name\": file,\n",
    "                    \"Predicted Class\": CLASS_NAMES[predicted_class],\n",
    "                    \"RNN Output\": rnn_output[0].tolist(),\n",
    "                    \"Final Output\": final_output[0].tolist(),\n",
    "                    \"CNN_Frame1-5_First10Vals\": cnn_vectors_flat\n",
    "                })\n",
    "\n",
    "    # Prepare headers dynamically\n",
    "    cnn_headers = [f\"F{f+1}_CNN_feat_{i+1}\" for f in range(5) for i in range(10)]\n",
    "    headers = [\"Video Name\", \"Predicted Class\", \"RNN Output\", \"Final Output\"] + cnn_headers\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(output_csv, mode='w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        for row in results:\n",
    "            flat_row = {\n",
    "                \"Video Name\": row[\"Video Name\"],\n",
    "                \"Predicted Class\": row[\"Predicted Class\"],\n",
    "                \"RNN Output\": str(row[\"RNN Output\"]),\n",
    "                \"Final Output\": str(row[\"Final Output\"]),\n",
    "            }\n",
    "            for i, val in enumerate(row[\"CNN_Frame1-5_First10Vals\"]):\n",
    "                flat_row[cnn_headers[i]] = val\n",
    "            writer.writerow(flat_row)\n",
    "\n",
    "    print(f\"\\nâœ… All predictions logged with CNN data from 5 frames â†’ {output_csv}\")\n",
    "\n",
    "# Run it\n",
    "process_videos_and_log(VIDEO_DIR, OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c463da-78ce-4b17-a11b-ac8d96e4ac2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16s/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model (if not already defined)\n",
    "model = load_model('activity_recognition_model.keras')  # Ensure model is saved properly\n",
    "\n",
    "# Define class names\n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "\n",
    "def plot_confusion_matrix(model, X_test, y_test, class_names):\n",
    "    \"\"\"Plot the confusion matrix.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)  # Convert softmax outputs to class indices\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the confusion matrix\n",
    "plot_confusion_matrix(model, X_test, y_test, CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303a367f-3573-47c7-b05e-906d017ffd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚                                 â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ time_distributed                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,574,339</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,   â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚                                 â”‚ \u001b[38;5;34m3\u001b[0m)                     â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ time_distributed                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2048\u001b[0m)       â”‚    \u001b[38;5;34m23,587,712\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mTimeDistributed\u001b[0m)               â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ sequential (\u001b[38;5;33mSequential\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚     \u001b[38;5;34m2,574,339\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,379,915</span> (299.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,379,915\u001b[0m (299.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,108,931</span> (99.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,108,931\u001b[0m (99.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,120</span> (207.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m53,120\u001b[0m (207.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">52,217,864</span> (199.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m52,217,864\u001b[0m (199.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f90b3-009b-475d-87fd-837d8be69cc7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Video classification using ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bce811b-3d4d-46b5-bf5c-e376074c4585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 47s/step - accuracy: 0.2500 - loss: 16.6396 - val_accuracy: 0.3333 - val_loss: 63.8408\n",
      "Epoch 2/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 30s/step - accuracy: 0.3889 - loss: 34.4294 - val_accuracy: 0.0000e+00 - val_loss: 18.0607\n",
      "Epoch 3/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 36s/step - accuracy: 0.6944 - loss: 8.2382 - val_accuracy: 0.3333 - val_loss: 41.2258\n",
      "Epoch 4/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 35s/step - accuracy: 0.5556 - loss: 13.5898 - val_accuracy: 0.3333 - val_loss: 33.0536\n",
      "Epoch 5/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 36s/step - accuracy: 0.7917 - loss: 2.5563 - val_accuracy: 0.3333 - val_loss: 59.9799\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 21s/step\n",
      "Predicted activity: Burglary\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, TimeDistributed\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Global Configurations\n",
    "FRAME_COUNT=30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 3  \n",
    "\n",
    "# Dataset Preparation\n",
    "def extract_frames(video_path, output_size=IMAGE_SIZE):\n",
    "    \"\"\"Extract frames from a video and resize them.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def preprocess_data(video_paths, labels, start_frame=200, end_frame=230):\n",
    "    \"\"\"Preprocess videos into fixed frame count sequences from a specified range.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        frames = extract_frames(video_path)\n",
    "        if len(frames) > end_frame:  # Ensure there are enough frames\n",
    "            X.append(frames[start_frame:end_frame])  # Take frames from start_frame to end_frame\n",
    "            y.append(labels[i])\n",
    "    return np.array(X), to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "def build_cnn():\n",
    "    \"\"\"Build a CNN model for spatial feature extraction.\"\"\"\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Combined Spatio-Temporal Model without RNN\n",
    "def build_spatio_temporal_model(cnn, frame_count, num_classes):\n",
    "    \"\"\"Combine CNN for spatio-temporal analysis.\"\"\"\n",
    "    video_input = Input(shape=(frame_count, 224, 224, 3))\n",
    "    cnn_output = TimeDistributed(cnn)(video_input)\n",
    "    \n",
    "    # Flatten the output and add Dense layers for classification\n",
    "    x = tf.keras.layers.Flatten()(cnn_output)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=video_input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example Dataset\n",
    "video_paths = ['Downloads/dataset/Burglary/Burglary059_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary009_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary062_x264.mp4', \n",
    "               'Downloads/dataset/Burglary/Burglary083_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary089_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse016_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse018_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse022_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse032_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse041_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse044_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest030_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest033_x264.mp4',\n",
    "              'Downloads/dataset/Arrest/Arrest015_x264.mp4',\n",
    "              'Downloads/dataset/Arrest/Arrest011_x264.mp4']  # Replace with actual paths\n",
    "\n",
    "labels = [0, 0, 0,0, 0, 1,1,1, 1, 1, 1, 2, 2,2,2]  # Example: burglary (0), abuse (1), arrest (2)\n",
    "\n",
    "# Preprocess Data\n",
    "X, y = preprocess_data(video_paths, labels, start_frame=200, end_frame=230)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build Models\n",
    "cnn = build_cnn()\n",
    "model = build_spatio_temporal_model(cnn, FRAME_COUNT, NUM_CLASSES)\n",
    "\n",
    "# Train the Model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the Model\n",
    "model.save('activity_recognition_model1.keras')\n",
    "\n",
    "# Real-Time Detection\n",
    "def predict_activity(model, video_path):\n",
    "    \"\"\"Predict the activity in a video.\"\"\"\n",
    "    frames = extract_frames(video_path)\n",
    "    frames = frames[:FRAME_COUNT]\n",
    "    frames = np.expand_dims(frames, axis=0)\n",
    "    prediction = model.predict(frames)\n",
    "    return np.argmax(prediction)\n",
    "\n",
    "# Test Prediction\n",
    "test_video = 'Downloads/dataset/Burglary/Burglary059_x264.mp4'  # Replace with the path to your test video\n",
    "result = predict_activity(model, test_video)\n",
    "activity_labels = ['Burglary', 'Abuse', 'Arrest']\n",
    "print(f\"Predicted activity: {activity_labels[result]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7494b80-205a-40cd-a824-43ebce80cb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚                                 â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ time_distributed_1              â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61440</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,864,448</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,   â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚                                 â”‚ \u001b[38;5;34m3\u001b[0m)                     â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ time_distributed_1              â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2048\u001b[0m)       â”‚    \u001b[38;5;34m23,587,712\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mTimeDistributed\u001b[0m)               â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61440\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚     \u001b[38;5;34m7,864,448\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚           \u001b[38;5;34m387\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">94,251,403</span> (359.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m94,251,403\u001b[0m (359.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,399,427</span> (119.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,399,427\u001b[0m (119.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,120</span> (207.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m53,120\u001b[0m (207.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,798,856</span> (239.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m62,798,856\u001b[0m (239.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e738c4-418f-493a-888c-aabe1ad00e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22s/step - accuracy: 0.3333 - loss: 59.9799\n",
      "Test Accuracy: 33.33%\n",
      "Predicted activity: Abuse\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"activity_recognition_model1.keras\"  \n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Test video path\n",
    "test_video = \"Downloads/dataset/Abuse/Abuse016_x264.mp4\" \n",
    "\n",
    "# Predict activity using the trained model\n",
    "result = predict_activity(model, test_video)\n",
    "# Evaluate the Model on Test Data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the Accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "# Activity labels\n",
    "activity_labels = ['Burglary', 'Abuse', 'Arrest']\n",
    "print(f\"Predicted activity: {activity_labels[result]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69774db6-fcae-4857-9add-b9fea61b157e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Video classification using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7e591cf-87ea-4be2-9327-587aa193d141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 50s/step - accuracy: 0.2917 - loss: 63.7929 - val_accuracy: 0.3333 - val_loss: 82.7560\n",
      "Epoch 2/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 60s/step - accuracy: 0.2500 - loss: 58.0696 - val_accuracy: 0.3333 - val_loss: 13.3908\n",
      "Epoch 3/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 54s/step - accuracy: 0.2083 - loss: 45.7541 - val_accuracy: 0.3333 - val_loss: 1.3152\n",
      "Epoch 4/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 56s/step - accuracy: 0.4028 - loss: 1.2288 - val_accuracy: 0.3333 - val_loss: 1.1029\n",
      "Epoch 5/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 55s/step - accuracy: 0.4861 - loss: 2.5527 - val_accuracy: 0.3333 - val_loss: 1.3305\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "Predicted activity: Burglary\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, TimeDistributed, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "# Global Configurations\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 3  \n",
    "\n",
    "# Dataset Preparation\n",
    "def extract_frames(video_path, output_size=IMAGE_SIZE):\n",
    "    \"\"\"Extract frames from a video and resize them.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def preprocess_data(video_paths, labels, start_frame=150, end_frame=180):\n",
    "    \"\"\"Preprocess videos into fixed frame count sequences from a specified range.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        frames = extract_frames(video_path)\n",
    "        if len(frames) > end_frame:  # Ensure there are enough frames\n",
    "            X.append(frames[start_frame:end_frame])  # Take frames from start_frame to end_frame\n",
    "            y.append(labels[i])\n",
    "    return np.array(X), to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "def build_cnn():\n",
    "    \"\"\"Build a CNN model for spatial feature extraction using VGG16.\"\"\"\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Combined Spatio-Temporal Model without RNN\n",
    "def build_spatio_temporal_model(cnn, frame_count, num_classes):\n",
    "    \"\"\"Combine CNN for spatio-temporal analysis.\"\"\"\n",
    "    video_input = Input(shape=(frame_count, 224, 224, 3))\n",
    "    cnn_output = TimeDistributed(cnn)(video_input)\n",
    "    \n",
    "    # Flatten the output and add Dense layers for classification\n",
    "    x = Flatten()(cnn_output)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=video_input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example Dataset\n",
    "video_paths = ['Downloads/dataset/Burglary/Burglary059_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary009_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary062_x264.mp4', \n",
    "               'Downloads/dataset/Burglary/Burglary083_x264.mp4',\n",
    "               'Downloads/dataset/Burglary/Burglary089_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse016_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse018_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse022_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse032_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse041_x264.mp4',\n",
    "               'Downloads/dataset/Abuse/Abuse044_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest030_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest033_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest015_x264.mp4',\n",
    "               'Downloads/dataset/Arrest/Arrest011_x264.mp4']  # Replace with actual paths\n",
    "\n",
    "labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2]  # Example: burglary (0), abuse (1), arrest (2)\n",
    "\n",
    "# Preprocess Data\n",
    "X, y = preprocess_data(video_paths, labels, start_frame=150, end_frame=180)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build Models\n",
    "cnn = build_cnn()\n",
    "model = build_spatio_temporal_model(cnn, FRAME_COUNT, NUM_CLASSES)\n",
    "\n",
    "# Train the Model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the Model\n",
    "model.save('activity_recognition_model_vgg16.keras')\n",
    "\n",
    "# Real-Time Detection\n",
    "def predict_activity(model, video_path):\n",
    "    \"\"\"Predict the activity in a video.\"\"\"\n",
    "    frames = extract_frames(video_path)\n",
    "    frames = frames[:FRAME_COUNT]\n",
    "    frames = np.expand_dims(frames, axis=0)\n",
    "    prediction = model.predict(frames)\n",
    "    return np.argmax(prediction)\n",
    "\n",
    "# Test Prediction\n",
    "test_video = 'Downloads/dataset/Burglary/Burglary059_x264.mp4'  # Replace with the path to your test video\n",
    "result = predict_activity(model, test_video)\n",
    "activity_labels = ['Burglary', 'Abuse', 'Arrest']\n",
    "print(f\"Predicted activity: {activity_labels[result]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d50ecc80-314c-4ab6-8eef-2c8334e78902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.3333 - loss: 1.3305\n",
      "Test Accuracy: 33.33%\n",
      "Predicted activity: Burglary\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"activity_recognition_model_vgg16.keras\"  \n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Test video path\n",
    "test_video = \"Downloads/dataset/Burglary/Burglary059_x264.mp4\" \n",
    "\n",
    "# Predict activity using the trained model\n",
    "result = predict_activity(model, test_video)\n",
    "# Evaluate the Model on Test Data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the Accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "# Activity labels\n",
    "activity_labels = ['Burglary', 'Abuse', 'Arrest']\n",
    "print(f\"Predicted activity: {activity_labels[result]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501ab75-86e1-4bd2-8566-a7a74dfbb8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a1cecb-17d0-4be5-8990-cc3b3aff35ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 366s/step - accuracy: 1.0000 - loss: 1.0195 - val_accuracy: 1.0000 - val_loss: 0.4508\n",
      "Epoch 2/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - accuracy: 1.0000 - loss: 0.4450 - val_accuracy: 1.0000 - val_loss: 0.2578\n",
      "Epoch 3/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 26s/step - accuracy: 1.0000 - loss: 0.2564 - val_accuracy: 1.0000 - val_loss: 0.1467\n",
      "Epoch 4/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 23s/step - accuracy: 1.0000 - loss: 0.1456 - val_accuracy: 1.0000 - val_loss: 0.0863\n",
      "Epoch 5/5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 28s/step - accuracy: 1.0000 - loss: 0.0865 - val_accuracy: 1.0000 - val_loss: 0.0521\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m15512841536\\x1b[0m\\n\\nArguments received by Functional.call():\\n  â€¢ inputs=tf.Tensor(shape=(1, 30, 2048), dtype=float32)\\n  â€¢ training=False\\n  â€¢ mask=None'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 149\u001b[0m\n\u001b[1;32m    147\u001b[0m visualize_feature_maps(cnn, sample_video)\n\u001b[1;32m    148\u001b[0m cnn_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([cnn\u001b[38;5;241m.\u001b[39mpredict(video) \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m X_train])\n\u001b[0;32m--> 149\u001b[0m \u001b[43mvisualize_rnn_hidden_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m sample_frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(extract_frames(sample_video)[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    151\u001b[0m generate_cam(cnn, sample_frame)\n",
      "Cell \u001b[0;32mIn[8], line 99\u001b[0m, in \u001b[0;36mvisualize_rnn_hidden_states\u001b[0;34m(rnn_model, cnn_features)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Visualize hidden states of LSTM layers.\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m rnn_intermediate \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39mrnn_model\u001b[38;5;241m.\u001b[39minput, outputs\u001b[38;5;241m=\u001b[39mrnn_model\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moutput)  \u001b[38;5;66;03m# First LSTM layer\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_intermediate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m    102\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(hidden_states[\u001b[38;5;241m0\u001b[39m])  \u001b[38;5;66;03m# Plot hidden states for the first video\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/keras/src/ops/function.py:179\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[0;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[1;32m    177\u001b[0m output_tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs:\n\u001b[0;32m--> 179\u001b[0m     output_tensors\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtensor_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mpack_sequence_as(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_struct, output_tensors)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Exception encountered when calling Functional.call().\\n\\n\\x1b[1m15512841536\\x1b[0m\\n\\nArguments received by Functional.call():\\n  â€¢ inputs=tf.Tensor(shape=(1, 30, 2048), dtype=float32)\\n  â€¢ training=False\\n  â€¢ mask=None'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Global Configurations\n",
    "FRAME_COUNT = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 3  \n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "\n",
    "# Dataset Preparation\n",
    "def extract_frames(video_path, output_size=IMAGE_SIZE):\n",
    "    \"\"\"Extract frames from a video and resize them.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def preprocess_data(video_paths, labels, frame_count=FRAME_COUNT):\n",
    "    \"\"\"Preprocess videos into fixed frame count sequences.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        frames = extract_frames(video_path)\n",
    "        if len(frames) >= frame_count:\n",
    "            X.append(frames[:frame_count])\n",
    "            y.append(labels[i])\n",
    "    return np.array(X), to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "def build_cnn():\n",
    "    \"\"\"Build a CNN model for spatial feature extraction.\"\"\"\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# RNN Model for Temporal Analysis\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    \"\"\"Build an RNN for temporal feature analysis.\"\"\"\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),  \n",
    "        LSTM(256, return_sequences=True, name='lstm_1'),\n",
    "        LSTM(128, return_sequences=True, name='lstm_2'),\n",
    "        LSTM(64, return_sequences=False, name='lstm_3'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Combined Spatio-Temporal Model\n",
    "def build_spatio_temporal_model(cnn, rnn, frame_count, num_classes):\n",
    "    \"\"\"Combine CNN and RNN for spatio-temporal analysis.\"\"\"\n",
    "    video_input = Input(shape=(frame_count, 224, 224, 3))\n",
    "    cnn_output = TimeDistributed(cnn)(video_input)\n",
    "    rnn_output = rnn(cnn_output)\n",
    "    model = Model(inputs=video_input, outputs=rnn_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Feature Map Visualization for CNN\n",
    "def visualize_feature_maps(cnn_model, video_path, layer_name='conv5_block3_out'):\n",
    "    \"\"\"Visualize feature maps from a specific CNN layer.\"\"\"\n",
    "    frames = extract_frames(video_path)[:FRAME_COUNT]\n",
    "    frames = np.expand_dims(frames[0], axis=0)  # Select the first frame\n",
    "    \n",
    "    intermediate_model = Model(inputs=cnn_model.input, outputs=cnn_model.get_layer(layer_name).output)\n",
    "    feature_maps = intermediate_model.predict(frames)\n",
    "    \n",
    "    num_features = feature_maps.shape[-1]  # Number of filters\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i in range(min(num_features, 16)):  # Display first 16 feature maps\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Feature Maps from Layer: {layer_name}\")\n",
    "    plt.show()\n",
    "\n",
    "# RNN Hidden State Evolution\n",
    "def visualize_rnn_hidden_states(rnn_model, cnn_features):\n",
    "    \"\"\"Visualize hidden states of LSTM layers.\"\"\"\n",
    "    rnn_intermediate = Model(inputs=rnn_model.input, outputs=rnn_model.get_layer('lstm_1').output)  # First LSTM layer\n",
    "    hidden_states = rnn_intermediate.predict(cnn_features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(hidden_states[0])  # Plot hidden states for the first video\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Hidden State Activation\")\n",
    "    plt.title(\"LSTM Hidden State Evolution\")\n",
    "    plt.show()\n",
    "\n",
    "# Class Activation Maps (CAM)\n",
    "def generate_cam(model, img_array, layer_name='conv5_block3_out'):\n",
    "    \"\"\"Generate Class Activation Map (CAM).\"\"\"\n",
    "    class_weights = model.layers[-1].get_weights()[0]\n",
    "    final_conv_layer = model.get_layer(layer_name)\n",
    "    grad_model = Model(inputs=model.input, outputs=[final_conv_layer.output, model.output])\n",
    "    conv_output, predictions = grad_model.predict(img_array)\n",
    "    \n",
    "    class_activation = np.dot(conv_output[0], class_weights[:, np.argmax(predictions)])\n",
    "    \n",
    "    plt.imshow(class_activation, cmap='jet')\n",
    "    plt.title(\"Class Activation Map\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example Dataset\n",
    "video_paths = ['Downloads/dataset/Burglary/Burglary059_x264.mp4']  \n",
    "labels = [0]  \n",
    "\n",
    "# Preprocess Data\n",
    "X, y = preprocess_data(video_paths, labels, frame_count=FRAME_COUNT)\n",
    "if len(X) > 1:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = X, X, y, y  # Use all data for training\n",
    "\n",
    "# Build Models\n",
    "cnn = build_cnn()\n",
    "rnn = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "model = build_spatio_temporal_model(cnn, rnn, FRAME_COUNT, NUM_CLASSES)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the Model\n",
    "model.save('activity_recognition_model.keras')\n",
    "\n",
    "# Visualizations\n",
    "sample_video = 'Downloads/dataset/Burglary/Burglary059_x264.mp4'\n",
    "visualize_feature_maps(cnn, sample_video)\n",
    "cnn_features = np.array([cnn.predict(video) for video in X_train])\n",
    "visualize_rnn_hidden_states(rnn, cnn_features)\n",
    "sample_frame = np.expand_dims(extract_frames(sample_video)[0], axis=0)\n",
    "generate_cam(cnn, sample_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fcd3d6-23ed-4e89-9420-7b75ae991d40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1c8bf-5b1e-499b-9a4c-c74bc6be5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and preprocessing video data...\n",
      "Total video sequences generated: 328\n",
      "Starting training...\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, TimeDistributed, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Global Configurations\n",
    "FRAME_COUNT = 30\n",
    "STRIDE = 30\n",
    "IMAGE_SIZE = (224, 224)\n",
    "NUM_CLASSES = 3\n",
    "CLASS_NAMES = ['Burglary', 'Abuse', 'Arrest']\n",
    "\n",
    "# Dataset Preparation\n",
    "def extract_frames(video_path, output_size=IMAGE_SIZE):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"[ERROR] Cannot open video file: {video_path}\")\n",
    "        return []\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, output_size)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "def preprocess_data_sliding_window(video_paths, labels, frame_count=FRAME_COUNT, stride=STRIDE):\n",
    "    X, y = [], []\n",
    "    for i, video_path in enumerate(video_paths):\n",
    "        frames = extract_frames(video_path)\n",
    "        if len(frames) < frame_count:\n",
    "            print(f\"[WARNING] Skipping {video_path} â€” too short or unreadable.\")\n",
    "            continue\n",
    "        for start in range(0, len(frames) - frame_count + 1, stride):\n",
    "            clip = frames[start:start + frame_count]\n",
    "            X.append(clip)\n",
    "            y.append(labels[i])\n",
    "    return np.array(X), to_categorical(y, num_classes=NUM_CLASSES)\n",
    "\n",
    "# CNN Model for Spatial Feature Extraction\n",
    "def build_cnn():\n",
    "    \"\"\"Build a CNN model for spatial feature extraction.\"\"\"\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# RNN Model for Temporal Analysis\n",
    "def build_rnn(input_shape, num_classes):\n",
    "    \"\"\"Build an RNN for temporal feature analysis.\"\"\"\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        LSTM(256, return_sequences=True),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Combined Spatio-Temporal Model\n",
    "def build_spatio_temporal_model(cnn, rnn, frame_count, num_classes):\n",
    "    \"\"\"Combine CNN and RNN for spatio-temporal analysis.\"\"\"\n",
    "    video_input = Input(shape=(frame_count, 224, 224, 3))\n",
    "    cnn_output = TimeDistributed(cnn)(video_input)\n",
    "    rnn_output = rnn(cnn_output)\n",
    "    model = Model(inputs=video_input, outputs=rnn_output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Example Dataset\n",
    "video_paths = [\n",
    "    'Downloads/dataset/Burglary/Burglary009_x264.mp4',\n",
    "    'Downloads/dataset/Abuse/Abuse032_x264.mp4',\n",
    "    'Downloads/dataset/Arrest/Arrest015_x264.mp4',\n",
    "    'Downloads/dataset/Arrest/Arrest042_x264.mp4'\n",
    "]\n",
    "labels = [0, 1, 2, 2]\n",
    "\n",
    "# Preprocess Data using sliding window\n",
    "print(\"Extracting and preprocessing video data...\")\n",
    "X, y = preprocess_data_sliding_window(video_paths, labels, frame_count=FRAME_COUNT, stride=STRIDE)\n",
    "print(f\"Total video sequences generated: {X.shape[0]}\")\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build Models\n",
    "cnn = build_cnn()\n",
    "rnn = build_rnn(input_shape=(FRAME_COUNT, 2048), num_classes=NUM_CLASSES)\n",
    "model = build_spatio_temporal_model(cnn, rnn, FRAME_COUNT, NUM_CLASSES)\n",
    "\n",
    "# Train the Model\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(X_train, y_train, epochs=2 , batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the Model\n",
    "model.save('activity_recognition_model_fullvideo.keras')\n",
    "print(\"Model saved to 'activity_recognition_model_fullvideo.keras'\")\n",
    "\n",
    "# Plot Loss and Accuracy per Class\n",
    "def plot_training_history_per_class(history):\n",
    "    \"\"\"Plot the training loss and accuracy for each class.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i, class_name in enumerate(CLASS_NAMES):\n",
    "        plt.subplot(3, 2, i + 1)\n",
    "        plt.plot(history.history['loss'], label=f'{class_name} Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label=f'{class_name} Validation Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Loss for {class_name}')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3, 2, i + 4)\n",
    "        plt.plot(history.history['accuracy'], label=f'{class_name} Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label=f'{class_name} Validation Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Accuracy for {class_name}')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history_per_class(history)\n",
    "\n",
    "# Real-Time Single Video Prediction\n",
    "def predict_activity(model, video_path):\n",
    "    \"\"\"Predict the activity from the first 30 frames of a video (for quick test).\"\"\"\n",
    "    frames = extract_frames(video_path)\n",
    "    if len(frames) < FRAME_COUNT:\n",
    "        print(\"Video too short for prediction.\")\n",
    "        return None\n",
    "    frames = frames[:FRAME_COUNT]\n",
    "    frames = np.expand_dims(frames, axis=0)\n",
    "    prediction = model.predict(frames)\n",
    "    return np.argmax(prediction)\n",
    "\n",
    "# Test Prediction\n",
    "test_video = 'Downloads/dataset/Burglary/Burglary059_x264.mp4'\n",
    "result = predict_activity(model, test_video)\n",
    "if result is not None:\n",
    "    print(f\"Predicted activity: {CLASS_NAMES[result]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42308000-d622-4323-b6b3-ac9e35dd23d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33s/step - accuracy: 1.0000 - loss: 0.0034\n",
      "Test Accuracy: 100.00%\n",
      "Predicted activity: Abuse\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"activity_recognition_model_fullvideo.keras\"  \n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Test video path\n",
    "test_video = \"Downloads/dataset/Burglary/Burglary062_x264.mp4\" \n",
    "\n",
    "# Predict activity using the trained model\n",
    "result = predict_activity(model, test_video)\n",
    "# Evaluate the Model on Test Data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the Accuracy\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "# Activity labels\n",
    "activity_labels = ['Burglary', 'Abuse', 'Arrest']\n",
    "print(f\"Predicted activity: {activity_labels[result]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e4de7-518e-46e6-bfa0-74183b698701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
